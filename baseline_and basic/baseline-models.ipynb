{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":2764486,"sourceType":"datasetVersion","datasetId":1686903},{"sourceId":12943547,"sourceType":"datasetVersion","datasetId":8105738}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Baseline Models Training\n\nThis notebook implements training for baseline models:\n- ResNet-50\n- DenseNet-121\n- Swin Transformer\n\nThese will serve as comparison baselines for the kidney ultrasound diagnosis task.","metadata":{}},{"cell_type":"code","source":"import sys\nfrom pathlib import Path\nsys.path.append(\"/kaggle/input/renalfiles\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-02T18:41:13.918637Z","iopub.execute_input":"2025-09-02T18:41:13.918945Z","iopub.status.idle":"2025-09-02T18:41:13.924849Z","shell.execute_reply.started":"2025-09-02T18:41:13.918924Z","shell.execute_reply":"2025-09-02T18:41:13.924331Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Import required libraries\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.transforms as transforms\nimport torchvision.models as models\nfrom torch.utils.data import DataLoader\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport os\nimport sys\n\nfrom dataloader import KidneyDataset, create_datasets, create_dataloaders, analyze_dataset\nfrom utils import calculate_metrics","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-02T18:41:13.926101Z","iopub.execute_input":"2025-09-02T18:41:13.926872Z","iopub.status.idle":"2025-09-02T18:41:28.228619Z","shell.execute_reply.started":"2025-09-02T18:41:13.926850Z","shell.execute_reply":"2025-09-02T18:41:28.227997Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Configuration\nBATCH_SIZE = 32\nLEARNING_RATE = 0.001\nEPOCHS = 50\nNUM_CLASSES = 4  # Adjust based on your dataset\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nprint(f\"Using device: {DEVICE}\")\nprint(f\"Batch size: {BATCH_SIZE}\")\nprint(f\"Learning rate: {LEARNING_RATE}\")\nprint(f\"Epochs: {EPOCHS}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-02T18:41:46.592165Z","iopub.execute_input":"2025-09-02T18:41:46.592659Z","iopub.status.idle":"2025-09-02T18:41:46.692646Z","shell.execute_reply.started":"2025-09-02T18:41:46.592631Z","shell.execute_reply":"2025-09-02T18:41:46.691974Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load datasets using the existing dataloader functions\n# Option 1: Use the existing create_datasets function\ntry:\n    # This assumes your data is organized in the standard format\n    datasets = create_datasets('/kaggle/input/ct-kidney-dataset-normal-cyst-tumor-and-stone/CT-KIDNEY-DATASET-Normal-Cyst-Tumor-Stone/CT-KIDNEY-DATASET-Normal-Cyst-Tumor-Stone')  # Adjust path as needed\n    train_dataset, val_dataset, test_dataset = datasets['train'], datasets['val'], datasets['test']\n    \n    print(f\"Using existing data splits:\")\n    print(f\"Training samples: {len(train_dataset)}\")\n    print(f\"Validation samples: {len(val_dataset)}\")\n    print(f\"Test samples: {len(test_dataset)}\")\n    \nexcept Exception as e:\n    print(f\"Could not load existing datasets: {e}\")\n    print(\"Please update the data path or create your own dataset loading code\")\n    \n    # Fallback: Manual dataset creation with transforms\n    train_transform = transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.RandomHorizontalFlip(p=0.5),\n        transforms.RandomRotation(10),\n        transforms.ColorJitter(brightness=0.2, contrast=0.2),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n    \n    val_transform = transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n    \n    # Create datasets manually - replace paths with your actual data paths\n    train_dataset = KidneyDataset([], [], transform=train_transform)\n    val_dataset = KidneyDataset([], [], transform=val_transform)\n    \n    print(\"Please update the dataset paths and labels in the cell above\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-02T18:41:48.037218Z","iopub.execute_input":"2025-09-02T18:41:48.037898Z","iopub.status.idle":"2025-09-02T18:41:48.195200Z","shell.execute_reply.started":"2025-09-02T18:41:48.037866Z","shell.execute_reply":"2025-09-02T18:41:48.194392Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create data loaders\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n\nprint(f\"Training batches: {len(train_loader)}\")\nprint(f\"Validation batches: {len(val_loader)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-02T18:41:52.856689Z","iopub.execute_input":"2025-09-02T18:41:52.857229Z","iopub.status.idle":"2025-09-02T18:41:52.862626Z","shell.execute_reply.started":"2025-09-02T18:41:52.857201Z","shell.execute_reply":"2025-09-02T18:41:52.861892Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define baseline models\ndef get_resnet50(num_classes):\n    \"\"\"ResNet-50 baseline model\"\"\"\n    model = models.resnet50(pretrained=True)\n    model.fc = nn.Linear(model.fc.in_features, num_classes)\n    return model\n\ndef get_densenet121(num_classes):\n    \"\"\"DenseNet-121 baseline model\"\"\"\n    model = models.densenet121(pretrained=True)\n    model.classifier = nn.Linear(model.classifier.in_features, num_classes)\n    return model\n\ndef get_swin_transformer(num_classes):\n    \"\"\"Swin Transformer baseline model\"\"\"\n    try:\n        from torchvision.models import swin_t\n        model = swin_t(pretrained=True)\n        model.head = nn.Linear(model.head.in_features, num_classes)\n        return model\n    except ImportError:\n        print(\"Swin Transformer not available in this torchvision version\")\n        print(\"Using ResNet-50 as fallback\")\n        return get_resnet50(num_classes)\n\n# Model selection\nmodels_dict = {\n    'resnet50': get_resnet50(NUM_CLASSES),\n    'densenet121': get_densenet121(NUM_CLASSES),\n    'swin_transformer': get_swin_transformer(NUM_CLASSES)\n}\n\nprint(\"Available models:\")\nfor name in models_dict.keys():\n    print(f\"- {name}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-02T18:48:27.737003Z","iopub.execute_input":"2025-09-02T18:48:27.737272Z","iopub.status.idle":"2025-09-02T18:48:28.935561Z","shell.execute_reply.started":"2025-09-02T18:48:27.737251Z","shell.execute_reply":"2025-09-02T18:48:28.934885Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"patience = 5","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-02T18:48:33.836863Z","iopub.execute_input":"2025-09-02T18:48:33.837124Z","iopub.status.idle":"2025-09-02T18:48:33.840599Z","shell.execute_reply.started":"2025-09-02T18:48:33.837106Z","shell.execute_reply":"2025-09-02T18:48:33.839952Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Training function\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom tqdm import tqdm\n\ndef train_model(model, train_loader, val_loader, epochs, device, model_name, patience=7):\n    \"\"\"Train a baseline model with early stopping\"\"\"\n    model = model.to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5)\n    \n    train_losses = []\n    val_losses = []\n    val_accuracies = []\n    \n    best_val_acc = 0.0\n    best_val_loss = float(\"inf\")\n    epochs_no_improve = 0\n    \n    for epoch in range(epochs):\n    # for epoch in range(2):\n        # Training phase\n        model.train()\n        train_loss = 0.0\n        \n        for batch_idx, (data, targets) in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")):\n            data, targets = data.to(device), targets.to(device)\n            \n            optimizer.zero_grad()\n            outputs = model(data)\n            loss = criterion(outputs, targets)\n            loss.backward()\n            optimizer.step()\n            \n            train_loss += loss.item()\n        \n        # Validation phase\n        model.eval()\n        val_loss = 0.0\n        correct = 0\n        total = 0\n        \n        with torch.no_grad():\n            for data, targets in val_loader:\n                data, targets = data.to(device), targets.to(device)\n                outputs = model(data)\n                loss = criterion(outputs, targets)\n                val_loss += loss.item()\n                \n                _, predicted = torch.max(outputs.data, 1)\n                total += targets.size(0)\n                correct += (predicted == targets).sum().item()\n        \n        train_loss /= len(train_loader)\n        val_loss /= len(val_loader)\n        val_acc = 100.0 * correct / total\n        \n        train_losses.append(train_loss)\n        val_losses.append(val_loss)\n        val_accuracies.append(val_acc)\n        \n        scheduler.step(val_loss)\n        \n        print(f\"Epoch {epoch+1}/{epochs}:\")\n        print(f\"  Train Loss: {train_loss:.4f}\")\n        print(f\"  Val Loss: {val_loss:.4f}\")\n        print(f\"  Val Acc: {val_acc:.2f}%\")\n        \n        # Save best model\n        if val_acc > best_val_acc:\n            best_val_acc = val_acc\n            torch.save(model.state_dict(), f'best_{model_name}_model.pth')\n            print(f\"  New best model saved with accuracy: {best_val_acc:.2f}%\")\n        \n        # Early stopping logic\n        if val_loss < best_val_loss - 1e-4:  # delta to avoid tiny fluctuations\n            best_val_loss = val_loss\n            epochs_no_improve = 0\n        else:\n            epochs_no_improve += 1\n\n        if epochs_no_improve >= patience:\n            print(f\"Early stopping triggered after {epoch+1} epochs.\")\n            break\n        \n        print(\"-\" * 50)\n    \n    return train_losses, val_losses, val_accuracies, best_val_acc\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-02T18:48:35.991925Z","iopub.execute_input":"2025-09-02T18:48:35.992183Z","iopub.status.idle":"2025-09-02T18:48:36.002252Z","shell.execute_reply.started":"2025-09-02T18:48:35.992163Z","shell.execute_reply":"2025-09-02T18:48:36.001465Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Train all baseline models\nresults = {}\n\nfor model_name, model in models_dict.items():\n    print(f\"\\n{'='*60}\")\n    print(f\"Training {model_name.upper()}\")\n    print(f\"{'='*60}\")\n    \n    train_losses, val_losses, val_accuracies, best_acc = train_model(\n        model, train_loader, val_loader, 2, DEVICE, model_name\n        # model, train_loader, val_loader, EPOCHS, DEVICE, model_name\n    )\n    \n    results[model_name] = {\n        'train_losses': train_losses,\n        'val_losses': val_losses,\n        'val_accuracies': val_accuracies,\n        'best_accuracy': best_acc\n    }\n    \n    print(f\"\\nCompleted training {model_name}\")\n    print(f\"Best validation accuracy: {best_acc:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-02T18:48:55.396806Z","iopub.execute_input":"2025-09-02T18:48:55.397313Z","iopub.status.idle":"2025-09-02T19:24:08.771391Z","shell.execute_reply.started":"2025-09-02T18:48:55.397286Z","shell.execute_reply":"2025-09-02T19:24:08.770681Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot training results\nfig, axes = plt.subplots(2, 2, figsize=(15, 10))\n\n# Training losses\nfor model_name, result in results.items():\n    axes[0, 0].plot(result['train_losses'], label=model_name)\naxes[0, 0].set_title('Training Loss')\naxes[0, 0].set_xlabel('Epoch')\naxes[0, 0].set_ylabel('Loss')\naxes[0, 0].legend()\naxes[0, 0].grid(True)\n\n# Validation losses\nfor model_name, result in results.items():\n    axes[0, 1].plot(result['val_losses'], label=model_name)\naxes[0, 1].set_title('Validation Loss')\naxes[0, 1].set_xlabel('Epoch')\naxes[0, 1].set_ylabel('Loss')\naxes[0, 1].legend()\naxes[0, 1].grid(True)\n\n# Validation accuracies\nfor model_name, result in results.items():\n    axes[1, 0].plot(result['val_accuracies'], label=model_name)\naxes[1, 0].set_title('Validation Accuracy')\naxes[1, 0].set_xlabel('Epoch')\naxes[1, 0].set_ylabel('Accuracy (%)')\naxes[1, 0].legend()\naxes[1, 0].grid(True)\n\n# Best accuracies comparison\nmodel_names = list(results.keys())\nbest_accs = [results[name]['best_accuracy'] for name in model_names]\naxes[1, 1].bar(model_names, best_accs)\naxes[1, 1].set_title('Best Validation Accuracy Comparison')\naxes[1, 1].set_ylabel('Accuracy (%)')\naxes[1, 1].tick_params(axis='x', rotation=45)\n\n# Add value labels on bars\nfor i, v in enumerate(best_accs):\n    axes[1, 1].text(i, v + 0.5, f'{v:.2f}%', ha='center', va='bottom')\n\nplt.tight_layout()\nplt.savefig('baseline_training_results.png', dpi=300, bbox_inches='tight')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-02T19:24:08.773273Z","iopub.execute_input":"2025-09-02T19:24:08.773587Z","iopub.status.idle":"2025-09-02T19:24:11.129798Z","shell.execute_reply.started":"2025-09-02T19:24:08.773564Z","shell.execute_reply":"2025-09-02T19:24:11.128871Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Summary of results\nprint(\"\\n\" + \"=\"*60)\nprint(\"BASELINE MODELS TRAINING SUMMARY\")\nprint(\"=\"*60)\n\nfor model_name, result in results.items():\n    print(f\"\\n{model_name.upper()}:\")\n    print(f\"  Best Validation Accuracy: {result['best_accuracy']:.2f}%\")\n    print(f\"  Final Training Loss: {result['train_losses'][-1]:.4f}\")\n    print(f\"  Final Validation Loss: {result['val_losses'][-1]:.4f}\")\n\n# Find best performing model\nbest_model = max(results.keys(), key=lambda x: results[x]['best_accuracy'])\nprint(f\"\\nBEST PERFORMING MODEL: {best_model.upper()}\")\nprint(f\"Accuracy: {results[best_model]['best_accuracy']:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-02T19:24:11.131088Z","iopub.execute_input":"2025-09-02T19:24:11.131304Z","iopub.status.idle":"2025-09-02T19:24:11.137082Z","shell.execute_reply.started":"2025-09-02T19:24:11.131288Z","shell.execute_reply":"2025-09-02T19:24:11.136522Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Baseline Models Evaluation\n\nThis evaluates the trained baseline models:\n- ResNet-50\n- DenseNet-121\n- Swin Transformer\n\nComprehensive evaluation including:\n- Test accuracy\n- Confusion matrix\n- Classification report\n- ROC curves\n- Model comparison","metadata":{}},{"cell_type":"code","source":"# Import required libraries\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.transforms as transforms\nimport torchvision.models as models\nfrom torch.utils.data import DataLoader\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import (\n    confusion_matrix, classification_report, \n    roc_curve, auc, accuracy_score,\n    precision_recall_fscore_support\n)\nfrom sklearn.preprocessing import label_binarize\nimport pandas as pd\nimport os\nimport sys\nfrom tqdm import tqdm\n\n# Add parent directory to path for imports\nfrom dataloader import KidneyDataset, create_datasets, create_dataloaders, analyze_dataset\nfrom utils import calculate_metrics\n\n# Set style\nplt.style.use('default')\nsns.set_palette(\"husl\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-02T19:24:11.138686Z","iopub.execute_input":"2025-09-02T19:24:11.138900Z","iopub.status.idle":"2025-09-02T19:24:11.168556Z","shell.execute_reply.started":"2025-09-02T19:24:11.138883Z","shell.execute_reply":"2025-09-02T19:24:11.167991Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Configuration\nBATCH_SIZE = 32\nNUM_CLASSES = 4  # Adjust based on your dataset\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Class names (adjust based on your dataset)\nCLASS_NAMES = ['Normal', 'Chronic Kidney Disease', 'Kidney Stone', 'Tumor']\n\nprint(f\"Using device: {DEVICE}\")\nprint(f\"Number of classes: {NUM_CLASSES}\")\nprint(f\"Class names: {CLASS_NAMES}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create test data loader\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\nprint(f\"Test batches: {len(test_loader)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-02T19:24:11.169191Z","iopub.execute_input":"2025-09-02T19:24:11.169434Z","iopub.status.idle":"2025-09-02T19:24:11.188645Z","shell.execute_reply.started":"2025-09-02T19:24:11.169402Z","shell.execute_reply":"2025-09-02T19:24:11.187934Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define model architectures (same as training)\ndef get_resnet50(num_classes):\n    model = models.resnet50(pretrained=False)\n    model.fc = nn.Linear(model.fc.in_features, num_classes)\n    return model\n\ndef get_densenet121(num_classes):\n    model = models.densenet121(pretrained=False)\n    model.classifier = nn.Linear(model.classifier.in_features, num_classes)\n    return model\n\ndef get_swin_transformer(num_classes):\n    try:\n        from torchvision.models import swin_t\n        model = swin_t(pretrained=False)\n        model.head = nn.Linear(model.head.in_features, num_classes)\n        return model\n    except ImportError:\n        print(\"Swin Transformer not available, using ResNet-50\")\n        return get_resnet50(num_classes)\n\n# Load trained models\nmodels_dict = {\n    'resnet50': get_resnet50(NUM_CLASSES),\n    'densenet121': get_densenet121(NUM_CLASSES),\n    'swin_transformer': get_swin_transformer(NUM_CLASSES)\n}\n\n# Load model weights\nloaded_models = {}\nfor model_name, model in models_dict.items():\n    try:\n        model.load_state_dict(torch.load(f'best_{model_name}_model.pth', map_location=DEVICE))\n        model.to(DEVICE)\n        model.eval()\n        loaded_models[model_name] = model\n        print(f\"Loaded {model_name} model successfully\")\n    except FileNotFoundError:\n        print(f\"Warning: {model_name} model weights not found\")\n\nprint(f\"\\nModels available for evaluation: {list(loaded_models.keys())}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-02T19:24:11.189556Z","iopub.execute_input":"2025-09-02T19:24:11.189864Z","iopub.status.idle":"2025-09-02T19:24:12.832549Z","shell.execute_reply.started":"2025-09-02T19:24:11.189841Z","shell.execute_reply":"2025-09-02T19:24:12.831845Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"CLASS_NAMES = ['Cyst', 'Normal', 'Stone', 'Tumor']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-02T19:27:47.972341Z","iopub.execute_input":"2025-09-02T19:27:47.973195Z","iopub.status.idle":"2025-09-02T19:27:47.977105Z","shell.execute_reply.started":"2025-09-02T19:27:47.973161Z","shell.execute_reply":"2025-09-02T19:27:47.976530Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Evaluation function\ndef evaluate_model(model, test_loader, device, model_name):\n    \"\"\"Comprehensive model evaluation\"\"\"\n    model.eval()\n    all_predictions = []\n    all_targets = []\n    all_probabilities = []\n    \n    print(f\"Evaluating {model_name}...\")\n    \n    with torch.no_grad():\n        for data, targets in tqdm(test_loader, desc=\"Evaluating\"):\n            data, targets = data.to(device), targets.to(device)\n            \n            outputs = model(data)\n            probabilities = F.softmax(outputs, dim=1)\n            _, predicted = torch.max(outputs, 1)\n            \n            all_predictions.extend(predicted.cpu().numpy())\n            all_targets.extend(targets.cpu().numpy())\n            all_probabilities.extend(probabilities.cpu().numpy())\n    \n    return np.array(all_predictions), np.array(all_targets), np.array(all_probabilities)\n\n# Evaluate all models\nevaluation_results = {}\n\nfor model_name, model in loaded_models.items():\n    predictions, targets, probabilities = evaluate_model(model, test_loader, DEVICE, model_name)\n    \n    # Calculate metrics using the imported function\n    metrics = calculate_metrics(targets, predictions, probabilities, CLASS_NAMES)\n    \n    evaluation_results[model_name] = {\n        'predictions': predictions,\n        'targets': targets,\n        'probabilities': probabilities,\n        'metrics': metrics\n    }\n    \n    print(f\"{model_name} - Test Accuracy: {metrics['accuracy']:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-02T19:27:53.972331Z","iopub.execute_input":"2025-09-02T19:27:53.973034Z","iopub.status.idle":"2025-09-02T19:29:06.591890Z","shell.execute_reply.started":"2025-09-02T19:27:53.973008Z","shell.execute_reply":"2025-09-02T19:29:06.590996Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Generate classification reports\nprint(\"\\n\" + \"=\"*80)\nprint(\"CLASSIFICATION REPORTS\")\nprint(\"=\"*80)\n\nfor model_name, results in evaluation_results.items():\n    print(f\"\\n{model_name.upper()}:\")\n    print(\"-\" * 50)\n    print(classification_report(\n        results['targets'], \n        results['predictions'], \n        target_names=CLASS_NAMES,\n        digits=4\n    ))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-02T19:29:57.292679Z","iopub.execute_input":"2025-09-02T19:29:57.293007Z","iopub.status.idle":"2025-09-02T19:29:57.316809Z","shell.execute_reply.started":"2025-09-02T19:29:57.292979Z","shell.execute_reply":"2025-09-02T19:29:57.316233Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot confusion matrices\nif evaluation_results:\n    fig, axes = plt.subplots(1, len(evaluation_results), figsize=(5*len(evaluation_results), 4))\n    if len(evaluation_results) == 1:\n        axes = [axes]\n\n    for idx, (model_name, results) in enumerate(evaluation_results.items()):\n        cm = confusion_matrix(results['targets'], results['predictions'])\n        \n        # Normalize confusion matrix\n        cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        \n        sns.heatmap(cm_normalized, \n                    annot=True, \n                    fmt='.3f', \n                    cmap='Blues',\n                    xticklabels=CLASS_NAMES,\n                    yticklabels=CLASS_NAMES,\n                    ax=axes[idx])\n        \n        axes[idx].set_title(f'{model_name.upper()}\\nAccuracy: {results[\"metrics\"][\"accuracy\"]:.3f}')\n        axes[idx].set_xlabel('Predicted')\n        axes[idx].set_ylabel('Actual')\n\n    plt.tight_layout()\n    plt.savefig('confusion_matrices.png', dpi=300, bbox_inches='tight')\n    plt.show()\nelse:\n    print(\"No models available for evaluation. Please check model loading.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-02T19:30:17.542264Z","iopub.execute_input":"2025-09-02T19:30:17.542590Z","iopub.status.idle":"2025-09-02T19:30:19.262312Z","shell.execute_reply.started":"2025-09-02T19:30:17.542559Z","shell.execute_reply":"2025-09-02T19:30:19.261684Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ROC Curves (for multi-class classification)\nif evaluation_results:\n    plt.figure(figsize=(12, 8))\n\n    for model_name, results in evaluation_results.items():\n        # Binarize the targets for multi-class ROC\n        targets_bin = label_binarize(results['targets'], classes=range(NUM_CLASSES))\n        probabilities = results['probabilities']\n        \n        # Compute ROC curve and AUC for each class\n        fpr = {}\n        tpr = {}\n        roc_auc = {}\n        \n        for i in range(NUM_CLASSES):\n            fpr[i], tpr[i], _ = roc_curve(targets_bin[:, i], probabilities[:, i])\n            roc_auc[i] = auc(fpr[i], tpr[i])\n        \n        # Compute micro-average ROC curve and AUC\n        fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(targets_bin.ravel(), probabilities.ravel())\n        roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n        \n        # Plot micro-average ROC curve\n        plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n                 label=f'{model_name} (AUC = {roc_auc[\"micro\"]:.3f})',\n                 linewidth=2)\n\n    plt.plot([0, 1], [0, 1], 'k--', linewidth=2, label='Random')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('ROC Curves - Baseline Models Comparison')\n    plt.legend(loc=\"lower right\")\n    plt.grid(True, alpha=0.3)\n    plt.savefig('roc_curves_comparison.png', dpi=300, bbox_inches='tight')\n    plt.show()\nelse:\n    print(\"No models available for ROC curve analysis.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-02T19:30:24.512527Z","iopub.execute_input":"2025-09-02T19:30:24.512824Z","iopub.status.idle":"2025-09-02T19:30:25.377116Z","shell.execute_reply.started":"2025-09-02T19:30:24.512795Z","shell.execute_reply":"2025-09-02T19:30:25.376449Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Per-class performance analysis\nif evaluation_results:\n    performance_data = []\n\n    for model_name, results in evaluation_results.items():\n        precision, recall, f1, support = precision_recall_fscore_support(\n            results['targets'], results['predictions'], average=None\n        )\n        \n        for i, class_name in enumerate(CLASS_NAMES):\n            performance_data.append({\n                'Model': model_name,\n                'Class': class_name,\n                'Precision': precision[i],\n                'Recall': recall[i],\n                'F1-Score': f1[i],\n                'Support': support[i]\n            })\n\n    performance_df = pd.DataFrame(performance_data)\n\n    # Plot per-class performance\n    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n    metrics = ['Precision', 'Recall', 'F1-Score']\n\n    for idx, metric in enumerate(metrics):\n        pivot_df = performance_df.pivot(index='Class', columns='Model', values=metric)\n        pivot_df.plot(kind='bar', ax=axes[idx], width=0.8)\n        axes[idx].set_title(f'{metric} by Class')\n        axes[idx].set_xlabel('Class')\n        axes[idx].set_ylabel(metric)\n        axes[idx].legend(title='Model')\n        axes[idx].tick_params(axis='x', rotation=45)\n        axes[idx].grid(True, alpha=0.3)\n\n    plt.tight_layout()\n    plt.savefig('per_class_performance.png', dpi=300, bbox_inches='tight')\n    plt.show()\n\n    # Display performance table\n    print(\"\\nPER-CLASS PERFORMANCE TABLE:\")\n    print(performance_df.round(4))\nelse:\n    print(\"No models available for per-class analysis.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-02T19:30:30.337692Z","iopub.execute_input":"2025-09-02T19:30:30.338324Z","iopub.status.idle":"2025-09-02T19:30:32.144163Z","shell.execute_reply.started":"2025-09-02T19:30:30.338302Z","shell.execute_reply":"2025-09-02T19:30:32.143549Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Overall model comparison\nif evaluation_results:\n    comparison_data = []\n\n    for model_name, results in evaluation_results.items():\n        metrics = results['metrics']\n        \n        comparison_data.append({\n            'Model': model_name,\n            'Accuracy': metrics['accuracy'],\n            'Precision (Macro)': metrics['macro_precision'],\n            'Recall (Macro)': metrics['macro_recall'],\n            'F1-Score (Macro)': metrics['macro_f1'],\n            'Precision (Weighted)': metrics['weighted_precision'],\n            'Recall (Weighted)': metrics['weighted_recall'],\n            'F1-Score (Weighted)': metrics['weighted_f1']\n        })\n\n    comparison_df = pd.DataFrame(comparison_data)\n    comparison_df = comparison_df.round(4)\n\n    print(\"\\n\" + \"=\"*100)\n    print(\"OVERALL MODEL COMPARISON\")\n    print(\"=\"*100)\n    print(comparison_df.to_string(index=False))\n\n    # Save results\n    comparison_df.to_csv('baseline_models_comparison.csv', index=False)\n    if 'performance_df' in locals():\n        performance_df.to_csv('baseline_models_per_class_performance.csv', index=False)\n\n    print(\"\\nResults saved to CSV files:\")\n    print(\"- baseline_models_comparison.csv\")\n    if 'performance_df' in locals():\n        print(\"- baseline_models_per_class_performance.csv\")\nelse:\n    print(\"No models available for comparison.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-02T19:30:37.612458Z","iopub.execute_input":"2025-09-02T19:30:37.613216Z","iopub.status.idle":"2025-09-02T19:30:37.635284Z","shell.execute_reply.started":"2025-09-02T19:30:37.613192Z","shell.execute_reply":"2025-09-02T19:30:37.634717Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Model ranking and best model identification\nif evaluation_results:\n    print(\"\\n\" + \"=\"*80)\n    print(\"MODEL RANKING\")\n    print(\"=\"*80)\n\n    # Rank by different metrics\n    ranking_metrics = ['Accuracy', 'F1-Score (Macro)', 'F1-Score (Weighted)']\n\n    for metric in ranking_metrics:\n        if metric in comparison_df.columns:\n            ranked = comparison_df.sort_values(metric, ascending=False)\n            print(f\"\\nRanking by {metric}:\")\n            for idx, (_, row) in enumerate(ranked.iterrows(), 1):\n                print(f\"  {idx}. {row['Model'].upper()}: {row[metric]:.4f}\")\n\n    # Overall best model (average ranking)\n    comparison_df['Average_Score'] = comparison_df[['Accuracy', 'F1-Score (Macro)', 'F1-Score (Weighted)']].mean(axis=1)\n    best_model = comparison_df.loc[comparison_df['Average_Score'].idxmax(), 'Model']\n\n    print(f\"\\n OVERALL BEST PERFORMING MODEL: {best_model.upper()}\")\n    print(f\"Average Score: {comparison_df.loc[comparison_df['Model'] == best_model, 'Average_Score'].values[0]:.4f}\")\n\n    # Performance insights\n    print(\"\\n\" + \"=\"*80)\n    print(\"PERFORMANCE INSIGHTS\")\n    print(\"=\"*80)\n\n    best_acc_model = comparison_df.loc[comparison_df['Accuracy'].idxmax(), 'Model']\n    best_f1_model = comparison_df.loc[comparison_df['F1-Score (Macro)'].idxmax(), 'Model']\n\n    print(f\" Highest Accuracy: {best_acc_model.upper()} ({comparison_df['Accuracy'].max():.4f})\")\n    print(f\" Best F1-Score (Macro): {best_f1_model.upper()} ({comparison_df['F1-Score (Macro)'].max():.4f})\")\n\n    # Performance differences\n    acc_diff = comparison_df['Accuracy'].max() - comparison_df['Accuracy'].min()\n    f1_diff = comparison_df['F1-Score (Macro)'].max() - comparison_df['F1-Score (Macro)'].min()\n\n    print(f\"\\n Performance Range:\")\n    print(f\"   Accuracy difference: {acc_diff:.4f}\")\n    print(f\"   F1-Score difference: {f1_diff:.4f}\")\n\n    if acc_diff < 0.05:\n        print(\"   → Models show similar performance levels\")\n    else:\n        print(\"   → Significant performance differences observed\")\nelse:\n    print(\"No models available for ranking analysis.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-02T19:30:42.477753Z","iopub.execute_input":"2025-09-02T19:30:42.478000Z","iopub.status.idle":"2025-09-02T19:30:42.493548Z","shell.execute_reply.started":"2025-09-02T19:30:42.477982Z","shell.execute_reply":"2025-09-02T19:30:42.492686Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!zip -r results ./","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}